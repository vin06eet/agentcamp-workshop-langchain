# Phase 2: GitHub Models Configuration

> â±ï¸ **Time to complete**: 10 minutes

In this phase, we'll set up access to GitHub Models - a **completely free** way to use powerful LLMs like GPT-4.1, Llama, and more. No credit card required!

---

## ğŸ¯ Learning Objectives

By the end of this phase, you will:
- Understand what GitHub Models offers and why we use it
- Generate a Personal Access Token (PAT)
- Configure your environment for API access
- Test the connection to verify it works
- Understand the LangChain ChatOpenAI integration

---

## ğŸ¤– What is GitHub Models?

GitHub Models is a free service that provides:
- Access to top LLMs (GPT-4.1, GPT-4o, Llama 3.3, Mistral, and more)
- OpenAI-compatible API endpoints
- No credit card or payment required
- Rate limits suitable for development and learning

### Why GitHub Models for This Workshop?

| Benefit | Description |
|---------|-------------|
| **Free** | No cost, no credit card required |
| **Easy Setup** | Just need a GitHub account you already have |
| **OpenAI Compatible** | Works with existing LangChain tools |
| **Multiple Models** | Access to various AI models |

### Available Models (as of 2026)

| Model | Provider | Best For |
|-------|----------|----------|
| `openai/gpt-4.1-nano` | OpenAI | Fast, efficient for learning |
| `openai/gpt-4.1` | OpenAI | Higher quality responses |
| `openai/gpt-4o` | OpenAI | Complex reasoning, best quality |
| `Meta-Llama-3.3-70B-Instruct` | Meta | Open-source alternative |

We'll use **`openai/gpt-4.1-nano`** in this workshop for fast responses.

---

## ğŸ”‘ Step 1: Generate a GitHub Personal Access Token

A Personal Access Token (PAT) is like a password that allows applications to authenticate with GitHub on your behalf.

### Navigate to Token Settings

1. Go to [github.com](https://github.com) and log in
2. Click your **profile picture** (top right)
3. Click **Settings**
4. Scroll down in the left sidebar to **Developer settings** (at the bottom)
5. Click **Personal access tokens** â†’ **Tokens (classic)**

Or go directly to: [github.com/settings/tokens](https://github.com/settings/tokens)

### Create a New Token

You can choose between **Fine-grained** or **Classic** tokens. For GitHub Models, either works, but the Classic token is simpler for this workshop,  but finegrained tokens are more secure for production use.

#### Fine-grained Token

Please follow the steps mentioned in the [AI Agent for Beginners setup](https://github.com/microsoft/ai-agents-for-beginners/blob/main/00-course-setup/README.md#set-up-for-samples-using-github-models).


#### Classic

1. Click **"Generate new token"** â†’ **"Generate new token (classic)"**

2. Fill in the form:
   - **Note**: `Workshop - LangChain Chainlit` (or any description)
   - **Expiration**: `7 days` (enough for the workshop)
   - **Scopes**: No scopes needed! Leave all boxes **unchecked**

{% hint style="warning" %}
**Important**: For GitHub Models, you don't need any scopes selected. A token with no scopes can still access the Models API.
{% endhint %}

3. Scroll down and click **"Generate token"**

4. **COPY THE TOKEN IMMEDIATELY!** 
   
   You'll see something like: `ghp_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx`
   
   âš ï¸ This is shown only once. If you lose it, you'll need to generate a new one.

---

## âš™ï¸ Step 2: Configure Your Environment

Now let's add the token to your `.env` file:

```bash
# Open the .env file in your editor and replace the placeholder
# It should look like this:

GITHUB_TOKEN=ghp_your_actual_token_here
WEATHER_API_KEY=your_weather_api_key_here
```

### Using the Terminal

```bash
# On macOS/Linux - replace with your actual token
sed -i '' 's/your_github_token_here/ghp_your_actual_token/' .env

# Or just edit with any text editor:
# VS Code: code .env
# Nano: nano .env
# Vim: vim .env
```

---

## ğŸ” Step 3: Understand the Architecture

Here's what happens when we call GitHub Models:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Your App      â”‚â”€â”€â”€â”€â–¶â”‚   GitHub Models API  â”‚â”€â”€â”€â”€â–¶â”‚  LLM Model  â”‚
â”‚   (LangChain)   â”‚     â”‚   (OpenAI Compatible)â”‚     â”‚  (GPT-4.1)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                        â”‚
         â”‚                        â”‚
    Uses GITHUB_TOKEN        Routes to model
    for authentication       you specified
```

### Key Details

| Setting | Value |
|---------|-------|
| **API Base URL** | `https://models.github.ai/inference` |
| **API Format** | OpenAI-compatible |
| **Authentication** | Bearer token (your GitHub PAT) |
| **Model Name** | e.g., `openai/gpt-4.1-nano` |

Because the API is OpenAI-compatible, we can use `langchain-openai` package with a custom base URL.

---

## ğŸ§ª Step 4: Test the Connection

Let's verify everything works with a simple test script.

Create a file called `test_github_models.py` in a new `phase-02` folder:

```bash
mkdir -p phase-02
cd phase-02
touch test_github_models.py
```

Now open `test_github_models.py` and add this code:

```python
"""
Phase 2: Test GitHub Models Connection
Run with: python test_github_models.py

This script verifies that your GitHub Models connection is working correctly.
It's the foundation for all subsequent phases.

Key Concepts Introduced:
- Loading environment variables with dotenv
- Creating a ChatOpenAI client configured for GitHub Models
- Making a simple LLM request
"""

import os
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI

# Load environment variables from .env file
# This reads the GITHUB_TOKEN from your .env file
load_dotenv()


def main():
    # Get the token from environment
    github_token = os.getenv("GITHUB_TOKEN")
    
    if not github_token or github_token == "your_github_token_here":
        print("âŒ Error: GITHUB_TOKEN not set in .env file")
        print("   Please add your GitHub token to the .env file")
        return
    
    print("ğŸ”„ Testing connection to GitHub Models...")
    
    # Create the LLM client using LangChain's ChatOpenAI
    # GitHub Models uses an OpenAI-compatible API, so we can use ChatOpenAI
    # but point it to GitHub's endpoint instead of OpenAI's
    llm = ChatOpenAI(
        model="openai/gpt-4.1-nano",  # Model format: provider/model-name
        api_key=github_token,          # Your GitHub PAT for authentication
        base_url="https://models.github.ai/inference",  # GitHub's inference endpoint
        temperature=0.7,               # Controls randomness (0=deterministic, 1=creative)
    )
    
    try:
        # Send a simple test message
        response = llm.invoke("Say 'Hello, Workshop!' and nothing else.")
        print(f"âœ… Success! Model responded: {response.content}")
        
    except Exception as e:
        print(f"âŒ Error: {e}")
        print("\nTroubleshooting:")
        print("- Make sure your GitHub token is valid")
        print("- Check your internet connection")
        print("- Verify the token hasn't expired")


if __name__ == "__main__":
    main()
```

### ğŸ” Understanding the Code

| Code | What It Does |
|------|--------------|
| `load_dotenv()` | Reads `.env` file and loads variables into environment |
| `os.getenv("GITHUB_TOKEN")` | Gets the token from environment |
| `ChatOpenAI(...)` | Creates a LangChain chat model client |
| `base_url=...` | Points to GitHub instead of OpenAI |
| `llm.invoke(...)` | Sends a message and gets a response |

Run the test:

```bash
python test_github_models.py
```

**Expected output**:
```
ğŸ”„ Testing connection to GitHub Models...
âœ… Success! Model responded: Hello, Workshop!
```

---

## ğŸ” Behind the Scenes: What Just Happened?

When you ran the test script:

1. **`load_dotenv()`** - Read the `.env` file and loaded `GITHUB_TOKEN` into the environment

2. **`ChatOpenAI(...)`** - Created an LLM client with:
   - Custom `base_url` pointing to GitHub's API
   - Your GitHub token as the `api_key`
   - Model set to `openai/gpt-4.1-nano`

3. **`llm.invoke(...)`** - Sent an HTTP POST request to:
   ```
   https://models.github.ai/inference/chat/completions
   ```
   With headers:
   ```
   Authorization: Bearer ghp_xxxxx
   Content-Type: application/json
   ```
   And body:
   ```json
   {
     "model": "openai/gpt-4.1-nano",
     "messages": [{"role": "user", "content": "Say 'Hello, Workshop!'..."}],
     "temperature": 0.7
   }
   ```

4. **Response** - GitHub routed the request to the GPT-4.1-nano model and returned the completion

---

## ğŸ“š Key Concepts to Remember

These concepts will be used in **every subsequent phase**:

| Concept | Code | Used For |
|---------|------|----------|
| Load env vars | `load_dotenv()` | Reading secrets from `.env` |
| Get env var | `os.getenv("GITHUB_TOKEN")` | Accessing the token |
| Create LLM | `ChatOpenAI(...)` | Making AI requests |
| Send message | `llm.invoke(message)` | Getting AI responses |

---

## ğŸ—‚ï¸ Current Project Structure

```
langchain-chainlit-workshop/
â”œâ”€â”€ .venv/
â”œâ”€â”€ .env                    # Now contains your GITHUB_TOKEN
â”œâ”€â”€ .gitignore
â”œâ”€â”€ requirements.txt
â””â”€â”€ phase-02/
    â””â”€â”€ test_github_models.py   # Test script
```

---

## âœ… Checkpoint: GitHub Models Connected

| Check | Result |
|-------|--------|
| Token generated | âœ“ Have a `ghp_xxx` token |
| Token saved | âœ“ In `.env` file |
| Connection works | âœ“ `test_github_models.py` shows success |

### ğŸ‰ Connection Working?

**Excellent!** You now have free access to powerful LLMs.

ğŸ‘‰ **Next up: [Phase 3: Your First Chainlit Chat App](03-chainlit-basics.md)**

---

## â“ Common Issues

### "401 Unauthorized" or "Authentication failed"
- Double-check your token is copied correctly (starts with `ghp_`)
- Make sure there are no extra spaces in the `.env` file
- Verify the token hasn't expired

### "Model not found" or "404"
- Check the model name is exactly `openai/gpt-4.1-nano`
- The format is `provider/model-name`

### "Rate limit exceeded"
- GitHub Models has rate limits for free tier
- Wait a minute and try again
- For workshops, the facilitator may have backup tokens

### Token appears in git history
- If you accidentally committed your token:
  1. Revoke it immediately on GitHub
  2. Generate a new one
  3. Update your `.env` file
  4. Add `.env` to `.gitignore` (already done if you followed Phase 1)
